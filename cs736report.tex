% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}
\graphicspath{{./figures/}}
%\hyphenation{op-tical net-works semi-conduc-tor pre-fe-tch
%             log-struc-tured}
             
%\usepackage[english]{babel}
%\usepackage{blindtext}
%
%\usepackage{etoolbox}
%\makeatletter
%\patchcmd{\maketitle}{\@copyrightspace}{}{}{}
%\makeatother



\begin{document}

\title{Analyzing and Improving ext4 Block Allocation Policies
\titlenote{This is a project report of course CS736 Advanced Operating System
at University of Wisconsin, Madison.}
}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
\alignauthor
Nathan Deisinger, Jun He\\
       \affaddr{Department of Computer Sciences}\\
       \affaddr{University of Wisconsin, Madison}\\
       \email{ndeisinger@wisc.edu, jhe@cs.wisc.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
We discuss the block allocation policies of ext4, a widely-used commodity filesystem. In doing so we explain how much of ext4's policy contains subtle effects that we suspect can degrade layout and performance.  Furthermore, we discuss how these policies are mismatched with modern uses of ext4 such as using it as the basis of a distributed file system, and provide a policy change that improves performance in this case.  Our work suggests that ext4 policy would be well-served by significant refinement.
\end{abstract}

%\keywords{Block allocation, ext4, file system} % NOT required for Proceedings

\section{Introduction}
The ext4 file system is a well-established commodity filesystem which builds upon the previous ext3 and ext2 filesystems and serves as the default in many Linux distributions, including the common consumer Ubuntu distribution.  In addition, as companies have come to use commodity hardware and software to build large-scale systems, ext4 has begun to see use as an underpinning of distributed file systems in corporate or research environments.
It is thus desirable that ext4 provide strong performance in a variety of situations.  Though there are many components to a filesystem that decide performance, a significant piece of the puzzle is the file system's block allocation policy – where on the disk it places data blocks for files.  Failure to lay out files in a contiguously or in line with logical file layout can lead to additional seeks due to fragmentation, reducing performance.
 
We chose to investigate ext4's block allocation policies in the hopes that they would prove to be well-defined and effective, so that programmers working on top of ext4 would be able to take advantage of ext4's behavior.  Unfortunately, we find that ext4's policies are complicated and not necessarily well-defined; for example, examination of the source code finds suggestions that aspects of ext4 policy are ad-hoc and only obscurely exposed to the user.
\footnote{ See http://pastebin.com/8iedq5ua for some of the source code; we could put one in as a figure.  Caption could be something like ``An excerpt from ext4's block allocation code.  The developers acknowledge a number of remaining policy issues'' for the first one and ``An excerpt from ext4's block allocation code.  We see little justification for the cutoffs used in this table, only a weak question of ``should this be tunable?''''}
 
In order to examine ext4's block allocation in practice, we developed a framework which generates configurable workloads which provide varying write patterns for a given file; by using this framework, we are able to expose some irregularities of ext4's block allocation policy.  In particular, we use this data to discuss ext4's policies and their lack of consistency in comparison with two other major file systems, XFS and Btrfs, and show that ext4's block allocation is significantly more varied for the same file written in various ways compared to these systems.
 
Examining ext4's policy in more detail reveals that a multitude of factors play into how a file has space allocated to it.  We discuss policies that decide block layout based on the current CPU, the size of the allocation, the size of the file, whether the allocation is the final block in the file, whether the request is aligned to an appropriate size, and whether the block is the head of a flex group (an ext4 structure to be discussed below).  In each case, we argue that the policy in question has at least the potential to cause poor performance or layout.
 
Finally, we focus our efforts and provide an example of how ext4 policy can affect a higher-level system.  We present the case of running the distributed Hadoop File System (HDFS) on top of ext4 and discuss how ext4's policies can degrade performance in such a system.  Though our proposed policy change is specialized, it provides recognizable improvement in the HDFS use case; however, it does stumble in more generalized benchmarks.  We believe this serves as an example of the difficulties of policy decisions and the need to maintain well-defined, understandable policies.
 
The rest of the paper continues as follows.  Section 2 provides an overview of related work on block allocation and ext4 policy.  Section 3 discusses the structure of ext4 as it relates to block allocation, as well as some high-level policies ext4 attempts to implement.  Section 4 discusses our methodology for studying general block allocation in ext4.  Section 5 compares ext4 to other file systems and discusses issues we have uncovered in ext4's block allocation policy.  Section 6 presents a concrete change to policy (the aforementioned HDFS case) and corresponding results.  Section 7 discusses future work and section 8 concludes.


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
\subsection{Parallels between memory allocation and ext4 disk allocation}
The potential for similar strategies being reused between disk allocation and memory allocation has been recognized in past work.  Many memory allocation mechanisms have two layers: a 'slab allocator' and a 'buddy allocator'.  The slab allocator can improve allocation efficiency and reduce internal fragmentation; within the allocator, a cache is defined which serves as storage for a certain type, such as semaphores, files and so on.  A 'slab' is taken to be the unit of space that a cache increases or shrinks by; generally, it consists of one or more pages. To reduce fragmentation, there are two kinds of slabs - small slabs and big slabs.  A small slab is exactly one page and is used for memory objects that is less than 1/8 of a page; if the object is larger, big slabs that can be more than one page are used\cite{bonwick1994slab}.
 
Ext4 has similar designs that aim to reduce fragmentation \cite{wilson1995dynamic}.  At a high level, blocks of small files and big files are allocated from different places - small file blocks are allocated from group preallocation space, where blocks of different files are packed together. In this layout, no holes are reserved.  Large file blocks are allocated from inode preallocation space; this space is reserved ahead of time for a specific file and cannot be used by other files.  This layout allows holes between files. The authors claim the policy improves locality and reduces fragmentation. Thus ext4's allocation policies serve as mirrors of the different slab sizes.  Similarly, as slabs may belong to different CPU, node, etc, preallocated disk blocks may belong to different CPUs, block groups, files, or directories.  Essentially, slab and preallocation are both pools with free and in-use spaces for management purpose. 


In the memory allocation model, a more fine-grained buddy allocator
 is present beneath the slab allocator.  In this buddy allocator, 
 the memory space is divided into sizes of order $2^{N}$.  For example, 
 a 1MB memory initially has $2^{8}$ free pages contiguously; to serve 
 requests, the free memory may be divided into sizes of smaller orders. 
 For example, to server a request of size 512KB ($2^{7}$ pages), the $2^{8}$ 
 free pages are divided into half (two parts, each has $2^{7}$ pages). This 
 first part is allocated to the program, and second half is left unused. 
 Later, when the space is freed, the allocator will check its neighbors 
 to see if they can be merged to a bigger free space. Ext4 also makes 
 use of a buddy allocator, but only for large requests that are aligned 
 to sizes of $2^{N}$. If the requests are not aligned, the disk is scanned 
 to collect enough free space to satisfy the request.  To minimize file 
 system fragmentation, ext4 has an online defragment tool which is 
 invoked by users periodically to reorganize files \cite{sato2007ext4}.

  
Justification for the concept of a buddy allocator can be found in the work of Wilson et. al.  In their 1995 survey of memory allocators, they reviewed a variety of possible allocation strategies and made note of the popularity of the buddy allocator due to its simplicity and speed~\cite{cao2008ext4}.  Several buddy system variations are introduced in~\cite{peterson1977buddy}; however, the high internal and external fragmentation that resulted made it unattractive to disk allocations. In \cite{koch1987disk}, Koch addressed the problem by offering the idea of allocating a file up to limited extents and reallocating non-optimal files. In ext4, as mentioned above, the problem is addressed via relying on buddy allocations only when requests are aligned among a power-of-two boundary.

\subsection{Known limitations in allocation}
Ext4 is recognized by its development community as an imperfect system.  Although it makes attempts to provide locality through methods such as grouping based on directory location, current discussions amongst the developers make note of issues such as an over-reliance on directory-based locality resulting in allocation of small, fragmented extents when large ones may be open one group over~\cite{FallocateCreating} and the need for users to be able to provide hints to the allocator as to the characteristics of the files they are writing through the fallocate() call~\cite{BlockReservation}.

\subsection{Comparison with Btrfs}
Ext4 has also been compared to Btrfs, a modern file system which makes use of B+ trees for its structure.  Although Btrfs makes use of delayed allocation like ext4, Btrfs makes use of 'allocation groups' - portions of space specifically marked for use on different types of data (eg. 'system' data which comprises the backbone of the file system, file metadata, and file data).  A tree describing free space in these groups is kept in memory and used to find just enough space in the data case or enough space plus a 64KB buffer in the metadata case.  Small files (<4KB) are in fact stored within the metadata blocks, rather than separately on disk\cite{kara2009ext4}.  Though the two filesystems display comparable performance (subject to some variations) \cite{kebede2012performance}, comparing these two file systems helps us in understanding the tradeoffs of block allocation.


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ext4 background}
\subsection{Data structure}
Ext4 builds off of the work of the previous ext file systems.  It carries over several of the fundamental data structures introduced in those systems, such as the superblock, bitmaps for block and inode allocation, and inode tables.  As in ext2 and ext3, the superblock maintains global information (such as block size) needed by the FS, and block and inode bitmaps indicate free blocks and entries in the inode tables, respectively.  In addition, the disk is divided into 'block groups' of a size defined in the superblock; in a default ext4 system with 4KB blocks, each block group is 16 MB.
 
Ext4 also introduces a new feature, the flexible block group (flex\_bg).  A single flex\_bg contains multiple standard block groups, with the block/inode bitmaps and inode tables of all block groups placed within the first block group.  Although this means the first block group in the flex\_bg uses more space on storing this metadata, the remaining groups need not store that metadata, potentially allowing better file contiguity due to no interruptions from metadata.  (Data blocks may still be interrupted by copies of the superblock; ext4 allows the user to specify how sparsely or densely the superblock is replicated.)\cite{ext4kernelnewbies}

\subsection{Extent-based file system}
In previous ext file systems, the data blocks of a file were pointed to by indirect blocks - metadata blocks containing pointers to other indirect blocks and ultimately the data blocks of the file itself.  Thus, metadata size for a file was proportional to the number of blocks.
 
 
Ext4 avoids this issue by making use of extents rather than indirect blocks for most files.  An extent is a contiguous space on disk consisting of one or more blocks, represented as a pair (start block, length).  In ext4, a 4KB block size allows for extents up to 128MB; for comparison, a non-extent-based system would require 32,678 pointers to reference the same number of blocks.  Thus ext4 is able to dramatically reduce the amount of metadata needed for individual files.

\subsection{Delayed allocation and multi-block allocation}
ext4 allocates disk blocks lazily. It buffers data in memory and allocates disk blocks when flushing. The flushing happe	ns when the user program calls flushing system calls, such as fsync(), sync(), or the operating system flushing daemon takes effect (e.g. pdflush).
 
Ext4 attempts to avoid unnecessary I/O by allocating disk blocks lazily.  Upon writes, it buffers the data in memory and allocates physical space on disk only when a flush is initiated through system calls such as fsync() or sync().
 
In tandem delayed allocation, ext4 also attempts to reduce I/O traffic and fragmentation by allocating multiple blocks at once rather than each block individually.
\cite{ext4kernelnewbies}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analyzer framework}
In order to examine ext4's block allocation policies, we needed to conduct a variety of file experiments.  In order to automate this process, we have built a framework designed to enable quick definition and execution of experiments; see Figure~\ref{fig:buddy-cache-prealloc}.

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 110mm 150mm 0mm, clip, width=80mm]
    	{framework}
    \caption{Analyzer Framework}
    \label{fig:framework}
\end{figure}

The Workload Generator generates workloads and save them to files formatted as one operation per line. Each operation is represented as:

pid;operation;filepath;parameters

For example, 0;write;./dir.001/file002;10;20 means ``the first process writes 20 bytes to offset 10 of ./dir.001/file002''

The Workload Player takes the workload files as input. It parses each line, executes the operations, and logs any errors or warnings to help ensure correctness.  To help speed up the process, workload files are often saved to a temporary file system in RAM and read by the player from there.
 
We separate the Workload Generator and Workload Player for flexibility. The generator is written in Python; the large number of packages available for the language make it simple to generate new workloads The player is written in C++ and makes use of MPI (Message Passing Interface). Using C++ allows us to interact with the operating system directly, whileMPI provides convenient synchronization between processes. Although it would be possible to combine the generator and player in a single C/C++ program, this approach would limit our flexibility because prototyping in C/C++ is more difficult and interacting with the test manager (written in Python) would require additional glue code.
 
The FS Manipulator is used to prepare the file system for testing; it handles tasks such as formatting, fragmenting, and aging the file system. The FS Monitor is the component that collects internal metadata of the file system. It makes use of ext4 utilities such as debugfs, dumpe2fs; for example, it uses dump\_extents in debugfs to retrieve physical locations of extents. The FS Monitor can save the collected information to files for offline analyzing (by R, a statistical programming language) or send it directly to the Analyzer. The Analyzer analyzes the information and sends the results to Test Manager as feedback; the Test Manager can then decide future tests based off of the results. Thus our components – the Test Manager, Workload Generator, Workload Player, File System, FS Monitor, and Analyzer - form a consistent feedback loop. We believe this loop  will serve well in performing sophisticated model checking in the future.
 


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Issues in ext4 policy}
\subsection{Comparison of block allocation in ext4, XFS, and Btrfs}

In order to explore issues in ext4 block allocation, we began by comparing block allocation between ext4, XFS, and Btrfs. In our most prominent  test, we write a single file with different patterns on an empty file system 4 GB in size.  For faster testing and to avoid any interference from the hardware, we ran these tests in a RAM disk.  
 
The write patterns vary in write order, fsync call frequency, and open-close-sync frequency. For our test, we divide a test file into three chunks: C0, C1, and C2. We can then vary our patterns in a variety of ways:
\begin{itemize}
	\item         With regards to write order, we can write in the order of C0-C1-C2, C0-C2-C1, and so on.
	\item       With regards fsync call frequency, we can do CX-fsync-CX-CX, CX-fsync-CX-fsync-CX, and so on.
	\item       With regards to open-sync-close frequency, we can do open-CX-CX-CX-close-sync, or open-CX-close-sync-open-CX-close-sync-open-CX-close-sync.
\end{itemize}
    
A given combination of write order, fsync call frequency, and open-close-sync frequency forms a complete write pattern. For example, open-writeC1-fsync-writeC3-close-sync-open-writeC1-close-sync is one complete pattern. Note that although both fsync and sync calls will force the file data to disk, they trigger different issues of block allocation.

For our comparison, we wrote files with different patterns and recorded the physical span of file data; we refer to this as the d-span.  We argue that d-span is a good indicator of file locality, as it provides a quantifiable measure of  how the data of a file is spread out on disk. Figure~\ref{fig:side-by-side-d-span} shows the resulting d-spans for different writing patterns. For each file size, we tested 192 different write patterns; each pattern corresponds to a data point on the graph of that file size. In addition to the individual data points, we display box plots to show the distribution of d-span across various patterns. For example, when the file size is 12KB, the box plot shows the median is at 2MB, and indeed most of the write patterns for 12KB files have a d-span larger than 2MB.  More importantly, this figure shows that ext4 block allocation is significantly more inconsistent than XFS and Btrfs. In the following subsections, we will discuss a number of ext4 policies we believe contribute to this inconsistent behavior.


\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 0mm 0mm 0mm, clip, width=80mm]
    	{side-by-side-d-span}
    \caption{cap}
    \label{fig:side-by-side-d-span}
\end{figure}



\subsection{Preallocation}
ext4 reserves disk space for files in memory, a strategy called 'preallocation' in memory. This means disk space is tagged as usable exclusively for certain files in the in-memory data structures; however, they are not tagged in any way on disk.
 
There are two types of preallocations - group preallocation and inode preallocation. Group preallocation is for small files; what constitutes a 'small file' can be defined as part of the ext4 configuration in /sys/fs/ext4/<disk>/mb\_stream\_req.   By default, ext4 defines 16 blocks to be the threshold; for ease of discussion, we use this default value alongside the default block size of 4KB and define a small file as 64KB in the below discussion.  Each CPU has its own group preallocation space shared by all small files. Inode preallocation, meanwhile, is used for large files and cannot be shared by multiple files; any inode preallocation takes place on a per-file basis.

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 130mm 150mm 0mm, clip, width=80mm]
    	{buddy-cache-prealloc}
    \caption{cap}
    \label{fig:buddy-cache-prealloc}
\end{figure}


Figure~\ref{fig:buddy-cache-prealloc} shows the space management system in memory.  Ext4 makes use of a buddy cache which marks free and non-free disk spaces and handles allocation decisions. Group and inode preallocation spaces are also allocated out of this buddy cache. When the file system is mounted, the free space and non-free space are identical to what the on-disk block bitmaps indicate. As files are created, some free spaces in the buddy cache are marked as non-free and tagged as preallocation spaces; file data blocks are then allocated out of group preallocation space, inode preallocation space, or free space in the buddy cache, depending on factors such as file size and block position in file.

\subsection{Normalization}
Normalization is the process of deciding how much extra space should be reserved for preallocation.  ext4 performs preallocation differently depending on the current file size. For small files, if currently there is no group preallocation, 512 blocks (default) will be allocated out of the buddy cache’s free space to serve as a new shared group preallocation . If the current file size is larger than 64KB (in the default case), the preallocation is done following Table X.

\begin{center}
    \begin{tabular}{ | l | p{5cm} |}
    \hline
    \bf{Cases} & \bf{Actions}  \\ \hline
    File size <=64KB & Small file, use group preallocation \\ \hline
    File size <=1MB & Preallocate $2^{ceiling(log(2, filesize))}$ \\ \hline
    File size <=4MB & Preallocate 2MB, align to $2\times X$ MB \\ \hline
    File size <=8MB & Preallocate 4MB, align to $4\times X$ MB \\ \hline
    Request size <=8MB & Preallocate 8MB, align to $8\times X$ MB  \\ \hline
    Others & No preallocation \\ \hline
    \end{tabular}
\end{center}

\subsection{Small file/big file threshold}
Though these normalization threshold seem well-defined, they mask an issue which occurs when a file crosses the threshold between 'small' and 'big'.  While the file is less than 64 KB in size, its data blocks are allocated from the group preallocation space; however, if it later grows past that size, it must place its blocks elsewhere on disk based on its inode preallocation.  As group preallocation spaces can be distant from other free space on the disk, this runs the risk of significant gaps in the file data; indeed, we observe just this issue in section 5.1, where this behavior is often responsible for the files being stripped across over gigabytes of disk.

\subsection{The Tail Effect}
ext4 allocates the last block of a file differently when the file is no longer opened by anyone; specifically, this last block will not trigger preallocation. We guess that the motivation of this behavior is that, since the status of a file is final (nobody can change it until next open), there is no need to reserve any additional space for it. This motivation is valid; however, when it is implemented, this causes inconsistent allocation for the tail and previous blocks, particularly in small files.

We consider the case of a small file with a logical hole in the file.  The blocks before the hole are allocated from group preallocation; however, the final block (tail) is allocated from buddy cache free space directly (not using existing group preallocation) since it does not need preallocation. For example, if we do:

open(file1)\\
write(file1, 0, 4096) //chunk 0\\
write(file1, 8192, 4096) //chunk 1\\
close(file1)

Chunk 0 is allocated from group preallocation space, while chunk 1 is allocated from outside of group preallocation space. Since the group preallocation space is usually 2MB,  Chunk 0 and Chunk 1 are potentially about 2MB away from each other. Indeed, most of the 2MB d-span in Figure~\ref{fig:side-by-side-d-span} is caused by this 'tail effect'. Similarly, big files can also exhibit this effect, as their final writes will have space allocated directly from the buddy cache rather than preallocated as the rest of the file was.

\subsection{Large file block group selection}
ext4 tries to allocate the first block of a small file to the same block group as its inode. However, for large files, the file system does not try to keep the data blocks of a file and its inode close; instead, it puts the first block of a big file to the block group where the previous big file block is allocated. The implication of this policy is that many new large files will be put into the same block group, and the extents of these files may become interleaved with each other if they grow together and are frequently synced.  We explore this behavior in more detail below in section 6.

\subsection{Small file CPU affinity}
In ext4, each CPU has its own group preallocation space; small files written by a given CPU have space allocated from the group preallocation of that CPU.  The motivation of attaching group preallocation to CPU was scalability – as each CPU has its own space and threads on different CPUs may not be likely to write to the same small file, this would reduce contention. However, this also implies that if the writer process switches to a different CPU, blocks newly written and forced to disk will be placed in a different preallocation space than their predecessors. We acknowledge this is not necessarily a common case, but it is still a potential source of fragmentation.

\subsection{Extent tree block allocation}
The extent tree block (ETB) is the block that holds pointers to data extents. In early versions of ext4, the ETB was allocated closely to the file's data extents; with this policy, the ETB could break the continuity of data blocks, as it would be mixed in with the data blocks. A patch in 2011 changed the policy to place the ETB close to inodes; however, we argue this introduces a new problem. In the case of large files, ext4 does not keep data blocks close to inodes; thus, data blocks are far away from both inodes and ETB. This potentially causes more, longer disk seeks. That said, as files rarely have a large number of extents, the number of ETBs are similarly unlikely to be large; thus we hope the number of long disk seeks should be low in real world.

\subsection{Alignment}
The alignment of a requested allocation makes a difference as well; in particular, requests of sizes that are aligned to $2^x$ blocks may be allocated differently than the ones that are not aligned. For example, aligned requests, such as a request of 512 blocks, cannot be allocated from the head block group of a flex group, but non-aligned requests, such as a request of 511 blocks, can. This may lead to inconsistent block allocations for a file if the file is written with both aligned and non-aligned block requests - some blocks may be placed in the head block group of the flex group, while some blocks may not not.

\subsection{Head block groups}
In the source code, the head block group of a flex group is intended to be used for directory data (as well as bitmaps, inode table) so that directory data has better locality. With this, commands like find will be faster, thanks to the short seek distance. However, the head block group of a flex group is occasionally also used for non-aligned data block requests, as mentioned above, potentially interleaving directory data and unrelated file data. We are not sure if this is an intended behavior or if they neglected to note such an effect; regardless, it is a counter-intuitive (to us) behavior and suggests a muddled policy.

\subsection{Implications for SSDs}
As a final note, we consider the case of SSDs.  While it is true that SSDs are good at random accesses, file locality can be important in a different way.  Recall that in SSDs, space is divided into blocks, which are further divided into pages. Data can be written directly into an empty page, but only whole blocks can be erased. Thus, if a file is spread widely, when we want to overwrite the file, we need to touch every block that has the file's data, triggering more read-erase-write cycles and shortening the life of the SSD.


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{HDFS and ext4: a case study}
\subsection{HBase+HDFS background}
As mentioned above, ext4 now serves not only as a consumer file system but also as the backbone of many larger corporate or distributed file systems; Google, for example, makes use of ext4 as the underlying file system of their servers running the Google File System (GFS), a distributed file system tailored to their needs\cite{ext4news}.  Though GFS is not publicly available, the Apache Hadoop project has established a similar file system known as the Hadoop File System (HDFS).  The Hadoop File System writes data to one or more data servers in the form of chunk files which are generally several dozen MB in size.  A separate server, the 'namenode', handles the mapping between chunks and files within the FS. 
 
 
 
\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 90mm 0mm 90mm, clip, width=80mm]
    	{stats-for-benchmarks-varmail}
    \caption{Benchmark results with the filebench varmail benchmark.  Our new change handles large files slightly better than default ext4, but falters elsewhere.}
    \label{fig:stats-for-benchmarks-varmail}
\end{figure}

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 90mm 0mm 90mm, clip, width=80mm]
    	{stats-for-benchmarks-fileserver}
    \caption{Benchmark results with the filebench fileserv benchmark.  Like varmail, our new change handles large files slightly better than default ext4, but performs worse in other cases.}
    \label{fig:stats-for-benchmarks-fileserver}
\end{figure}



\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 90mm 0mm 90mm, clip, width=80mm]
    	{stats-for-benchmarks-postmark}
    \caption{Benchmark results with the Postmark benchmark.  Regrettably, our change causes a notable drop in throughput for both reads and writes.}
    \label{fig:stats-for-benchmarks-postmark}
\end{figure}

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 90mm 0mm 90mm, clip, width=80mm]
    	{stats-for-benchmarks-hdfs}
    \caption{Benchmark results for our HDFS test.  Our new change sees notable improvement in this HDFS case compared to stock ext4.}
    \label{fig:stats-for-benchmarks-hdfs}
\end{figure}

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 0mm 0mm 0mm, clip, width=80mm]
    	{nochange-flat-fixed3}
    \caption{Layout of five HDFS chunk files under standard ext4 when durably, concurrently appended.  Notice the significant interleaving}
    \label{fig:nochange-flat-fixed3}
\end{figure}

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 0mm 0mm 0mm, clip, width=80mm]
    	{change-flat-fixed3}
    \caption{Layout of five HDFS chunk files under our modified ext4 when durably, concurrently appended.  The files are almost entirely contiguously laid out, with only one small extent in two of the files placed distant from the remainder.}
    \label{fig:change-flat-fixed3}
\end{figure}
 
 
In addition to HDFS, Apache provides the HBase project, a distributed database designed to run on top of an HDFS installation and provide much of the same functionality as Google's BigData database software.  Although HBase, like HDFS, is designed to rely on cross-server duplication to provide durability guarantees, the HBase community has recognized the need for programmers to be able to guarantee data has been durably written to at least one disk; thus, HBase is moving to make use of HDFS’ hsync() call, which serves as an HDFS equivalent of the POSIX fsync() call by synchronously committing data to the disk.
 
Unfortunately, this change has not yet been fully implemented by the community, although it is acknowledged as a critical issue\cite{hbaseissue}.  In the discussions below, we make use of a microbenchmark designed to simulate the act of multiple clients writing simultaneously to an HDFS share as HBase might.

\subsection{ext4 problem and proposed change}
In performing big-data work through HBase, we believe it is not unreasonable to expect that multiple nodes in a network may send significant amounts of data to a single HBase node concurrently.  If the designers of the system desire durability guarantees, they may make use of HDFS' hsync() capabilities, which in turn rely on ext4's fsync() implementation.  We thus consider how ext4 handles the case where many concurrent, large appends are being sent to the HDFS server.
 
In standard ext4, the file system attempts to group together large files by placing them in the same block group.  However, this behavior fails in our case; when multiple large files are concurrently, durably appended to, ext4 winds up interleaving the writes on the disk (see figure~\ref{fig:nochange-flat-fixed3}).  Although ext4's preallocation policies attempt to mitigate the issue by providing 8MB extents to each allocation, having many clients writing at once results in significant interleaving between files; this not only has immediate implications for performance, but runs the risk of increased framgentation as the file system ages.
 
We provide a simple policy change; rather than group large files together in the same block group, we attempt to place large files in a different group from the previous large write.  Our hope is that in doing so, we are able to give these large files room to grow without running the risk of interleaving.

\subsection{Evaluation}
As mentioned above, we originally hoped to make use of HBase functionality to provide a more realistic benchmark; unfortunately, as HBase support for durable writing has yet to be implemented , we were forced to create a microbenchmark of our own.
 
The microbenchmark first creates five processes that each write 32 64 MB files to the HDFS store, durably appending 1 MB at a time.  Once this is complete, five reader processes are launched which each then read back 32 of the files, spread across the disk.  This I/O is wholly sequential, in the hopes of matching common use cases for big data.
 
For these benchmarks, we made use of a machine running Ubuntu 12.04 with modified and unmodified Linux kernel 3.2.17.  The machine had a four-core Intel i5 processor clocked at 1.7GHz; the hard drive used was a Seagate SRD0SP0 with a rotation rate of 5400 RPM.  In addition, we use Hadoop version 2.2.0 running on a single machine, rather than a distributed setup, to examine the results of multiple writes hitting the same server simultaneously.
 
In Figure~\ref{fig:nochange-flat-fixed3}, we see that with the default ext4 policy HDFS' chunk files become heavily, if roughly evenly, interleaved on the disk.  Figure~\ref{fig:change-flat-fixed3} shows that with our change they are each allocated an appropriately-large extent and display minimal interleaving.  Although disk bandwidth dominates in situations of large sequential I/O, our new layout still provides a noticeable improvement in read times; reading back the files through those five threads takes an average of 647.23 seconds without our change (for a throughput of 126.57 Mb/s) and 595.40 seconds with (a throughput of 137.59 Mb/s), an 8.0\%  increase in speed (see Figure~\ref{fig:stats-for-benchmarks-hdfs}).  We suspect that in addition to the reduced number of seeks, the strong contiguity of the files enables ext4 and the hard disk to make better I/O scheduling decisions.

\subsection{Influence to general workload (general benchmarks)}
We acknowledge the limitations of such a benchmark; our policy change is designed precisely for these sorts of HDFS workloads.  Nevertheless, we were curious as to how our changes affected more general workloads when applied to ext4.
 
We made use of three different popular disk benchmarks: filebench's varmail and fileserver benchmarks, and the Postmark benchmark.  In the case of varmail and fileserver, we also varied the sizes of the files involved.
 
Our results are summarized in the Figure~\ref{fig:stats-for-benchmarks-postmark}~\ref{fig:stats-for-benchmarks-fileserver}~\ref{fig:stats-for-benchmarks-varmail}.  For the filebench benchmarks - that is, varmail and fileserver - we define the benchmark sizes as follows:

\begin{itemize}
	\item The 'default' benchmark is a run with the default settings for each benchmark.  In the varmail case, this is 1,000 files with a mean filesize of 16KB; in the fileserver case, this is 10,000 files with a mean filesize of 128KB.

	\item The 'medium' benchmark is a run with 500 files of an average 10 MB size,

	\item The 'large' benchmark is a run with 400 files of an average 40 MB size.
\end{itemize}

We present our results in  Figure~\ref{fig:stats-for-benchmarks-postmark}~\ref{fig:stats-for-benchmarks-fileserver}~\ref{fig:stats-for-benchmarks-varmail}.  We see a slight improvement in performance with our change on the large loads, a significant (13.5\%) decrease in performance on the varmail medium load, and significant decreases in performance on the default loads (13.3\% for varmail, 12.9\% for fileserv). 
 
For the postmark benchmark, we defined a run with 200 files ranging between 1 and 64 MB in size, which we then performed 1000 transactions on.  Again, we see our change has caused some loss in speed – namely, a 6.0\% decrease in read throughput and a 6.5\% decrease in write throughput.
 
Although time constraints prevent us from performing a more thorough analysis of these results, we suspect that these benchmarks may not necessarily make heavy use of the fsync() command; thus, ext4 is more free to group I/O as it desires.  In the standard ext4 setup, this would enable it to group large files in the same block group without running the risk of fragmentation; however, with our change, these files would be placed in a separate block group, thus causing additional seek time. [12]  We are uncertain as to why we see significant performance decreases in the default, small-file case for filebench; perhaps this serves as a warning that even a simple policy change can have unforeseen consequences.



%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future work}
Our work with ext4 has served largely as a survey and not as a comprehensive classification or rewrite.  A natural further step to take along this line of research would be to not only continue investigation of ext4 block allocation policies but to define a meaningful classification for its various facets which could then be used to more objectively compare its strategies with those of competing file systems.
 
ext4 itself could also make use of a more detailed survey or classification.  By commenting source code or creating documentation to better explain the various facets of ext4's block allocation policy, we would hope that developers would have a better grasp on how to design their programs to make best use of ext4, or to have a better idea of what to change in ext4 if they desire a more fine-tuned solution.
 
Finally, as we have discussed above, ext4's existing policies are yet imperfect, be that for specific cases (HDFS) or in more general ones (small file CPU affinity).  More subtle alterations to ext4 policy may yet provide better block layout and performance.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
ext4 is a complicated beast.  In making the most fundamental decision of a file system – where to place data on the disk – it takes into account a variety of factors and applies a number of complicated, if imperfect, policies to make its choice.  Ideally, ext4's policies would be open and easily-understandable; barring that, we hope they would at least be comprehensive and consistent.  We find them to be neither.  The sheer number of factors that go into determining block layout make ext4's policy a black box to users, and its behavior displays remarkable inconsistency compared to competing file systems such as XFS and Btrfs.
 
We find the behavior of ext4 and the results of these policies particularly concerning when we consider that ext4 is today being used beyond the confines of a basic user file system.  As ext4 is used in advanced systems such as distributed file systems, issues of policy seem likely to rear their heads in increasingly complicated and concerning ways.  Our investigation of HDFS's work case and how it interacts with ext4 leads to a simple policy change that, while helpful in HDFS' case, provides mixed results with more general workloads.  This result serves as a reminder both of the importance and the subtlety of policy decisions.
 
We do not mean to utterly disparage ext4.  It is complicated in the hopes of serving many purposes, and issues such as HDFS layout could be forgiven, given they fall outside of the scope of a traditional file system.  But its labyrinth policy decisions make understanding it a challenge, and adapting to it even more of one.  We hope that in the future, be it in later ext4 revisions or in different file systems altogether, developers recall that policy is as much a first-class citizen of design as implementation – and that users of their system desire and deserve a well-defined policy.


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
% That's all folks!
\end{document}
